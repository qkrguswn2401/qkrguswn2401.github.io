---
title: "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization"
date: 2021-10-03 08:26:28 -0400
categories: Paper Review
---
PEGAsus paper review

Abstract:
  Pre-training transformer with self-supervised objective on large text has shown great success on downstreaming NLP task.
  In PEGASUS, transformer-based encoder-decoder method is provided with a new self-supervised objective.  Importance sentences are removed or maksed from an input documents/sentences and generated together as one output sequences from the remaining sentneces similar to an extractive summarization.
  It has shown SOTA on 12 downstream summarization tasks measured by ROUGE scores.

Introduction:
  ![image](https://user-images.githubusercontent.com/36841216/137128025-ba4ed8dd-975e-4f18-ae8a-bf4da6daaa5a.png)

  As opposed to extractive summarization which simply compies some sentences from input, abstractive summarization may generate novel word.  Good abstractive summaries cover important information from input and are cohernet/fluent.
  sequence-to-sequence encoder-decoder based on RNNs and recently transformers have shown to be dominant in abstractive summarization.
  Recently, pre-trainined using self-supervised objectives on large text corposa have shown great success.  
  Masking whole sentences from a document and generating these gap-sentences from the rest of the document works well for pre-training objective, choosing important sentences outperforms lead or randomly selected sentences.
  This is called self-supervised Objective Gap Sentences Generation (GSG).
 
 Related Works:
  MASSS: maksed sequence-to-sequence generation that reconstructs a sentence fragment given the remaining part of the sentence; single sentence is randomply selected
  
  UniLM: jointly training on three objectives: unidirectional, bidirectional, sequence-to-sequence prediction.
  
  BART:  introdued denoising autoencoder to pretrain sequence-to-sequence models.  BART corrupts input text with an arbitrary noising functon and learn to reconsctruct the original sentence.
  
