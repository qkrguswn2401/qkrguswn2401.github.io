---
title: "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization"
date: 2021-10-03 08:26:28 -0400
categories: Paper Review
---
PEGAsus paper review

Abstract:
  Pre-training transformer with self-supervised objective on large text has shown great success on downstreaming NLP task.
  In PEGASUS, transformer-based encoder-decoder method is provided with a new self-supervised objective.  Importance sentences are removed or maksed from an input documents/sentences and generated together as one output sequences from the remaining sentneces similar to an extractive summarization.
  It has shown SOTA on 12 downstream summarization tasks measured by ROUGE scores.

Introduction:
  ![image](https://user-images.githubusercontent.com/36841216/137128025-ba4ed8dd-975e-4f18-ae8a-bf4da6daaa5a.png)

  As opposed to extractive summarization which simply compies some sentences from input, abstractive summarization may generate novel word.  Good abstractive summaries cover important information from input and are cohernet/fluent.
  sequence-to-sequence encoder-decoder based on RNNs and recently transformers have shown to be dominant in abstractive summarization.
  Recently, pre-trainined using self-supervised objectives on large text corposa have shown great success.  
  Masking whole sentences from a document and generating these gap-sentences from the rest of the document works well for pre-training objective, choosing important sentences outperforms lead or randomly selected sentences.
  This is called self-supervised Objective Gap Sentences Generation (GSG).
 
 Related Works:
  MASSS: maksed sequence-to-sequence generation that reconstructs a sentence fragment given the remaining part of the sentence; single sentence is randomply selected.  Inspired by BERT, MASS has encoder-decoder structure with masked sequence to sequence.  In encoder, input sentences are randomply masked and let decoder predict the maksed sentences.  showed a little improvement in langauge generation task.  The main difference from BERT is the masking.  It masks consecutive tokens or tokens in a row whereas BERT masks input based on some discrte distribution.
  ![image](https://user-images.githubusercontent.com/36841216/137245768-75372be2-4bd3-43dd-b158-b89677b71ee9.png)

  UniLM: jointly training on three objectives: unidirectional, bidirectional, sequence-to-sequence prediction.
  inputs: two segments in a sequence,  for uni: one segment.
  Every sequence starts with [sos] and ends with [eos]
  every word is tokenized into subwords by wordpiece.
  
  1. Unidirectional LM: uses one direction to predict next word.  Ex)  [x1,x2,x3 (Masked), x4], here we used previous x1,x2,x3 to predict the mask. Just likt the figure next, it calcuates attention values using previous tokens and itself.  ![image](https://user-images.githubusercontent.com/36841216/137248757-ea659fcc-d2b8-47a6-ac61-6441bbae4a1e.png)
  2. Bidirectional LM:   Like BERT, it uses looks at context from bidirection to predict the mask.  Because each token attends each other, it have richer context compared to the unidirecitonal LM.  It uses NSP for an additional objective. 
  3. Sequence-to-sequence LM:   In sequence-to-sequence method, two segments are split into source and target ([sos], t1,t2,t3, [eos],t3,t4,t5,t6[eos]).  Just like seq2seq, each source token attends each other,  target token only attends source token + previous target tokens.  
  
  BART:  introdued denoising autoencoder to pretrain sequence-to-sequence models.  BART corrupts input text with an arbitrary noising functon and learn to reconsctruct the original sentence. Noising function is text infilling, which single masks token replaces sampled span of text.  
 
 In contrast to BART, MASS, and UniLM, PEGASUS masks multiple whole sentences rather than smaller contious text spans.  Texts are chosen based on importance not randomly.  PEGASUS only reconstructs the masked tokens as a a single output tokens.  
 
 Pretraining Objective:
 Gap Sentences Generation (GSG):
  Idea: using pre-training objective that more resembles the downstream task would increase the performance of the model.  Given abstractive summarization, proposed pretraining objecive is convering summary of an input sentence.  Whole sentences from documents are selected and masked and concatenate the gap-sentences into a pseudo-summary.  GAP sentence ratio = the number of selected gap sentences to the total number of sentences similar to maks ratio.
  Three primary strategies for selecting m gap sentences: random, lead, principle.  
    Random: uniformly select m sentences at random.
    Lead: select the first m sentenecs.
    Principle: select top-m scored sentences according to importance (ROUGE)
  
  
