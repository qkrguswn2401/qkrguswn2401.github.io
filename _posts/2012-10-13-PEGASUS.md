---
title: "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization"
date: 2021-10-03 08:26:28 -0400
categories: Paper Review
---
PEGAsus paper review

Abstract:
  Pre-training transformer with self-supervised objective on large text has shown great success on downstreaming NLP task.
  In PEGASUS, transformer-based encoder-decoder method is provided with a new self-supervised objective.  Importance sentences are removed or maksed from an input documents/sentences and generated together as one output sequences from the remaining sentneces similar to an extractive summarization.
  It has shown SOTA on 12 downstream summarization tasks measured by ROUGE scores.

Introduction:
  ![image](https://user-images.githubusercontent.com/36841216/137128025-ba4ed8dd-975e-4f18-ae8a-bf4da6daaa5a.png)

  As opposed to extractive summarization which simply compies some sentences from input, abstractive summarization may generate novel word.  Good abstractive summaries cover important information from input and are cohernet/fluent.
  sequence-to-sequence encoder-decoder based on RNNs and recently transformers have shown to be dominant in abstractive summarization.
  Recently, pre-trainined using self-supervised objectives on large text corposa have shown great success.  
  Masking whole sentences from a document and generating these gap-sentences from the rest of the document works well for pre-training objective, choosing important sentences outperforms lead or randomly selected sentences.
  This is called self-supervised Objective Gap Sentences Generation (GSG).
 
 related works
  

Models:
  two novel ideas:
    soft-labeling method for context prediction
    blcok attention method that predicts the soft-labels.
  two important functons
    1.  Calculating the probability that a word in a passage belongs to the context
    2.  enabling the probability to reflect spatial locality between adjacent words.
    
  Soft-labeling for latent context
    Assumption: words near an answer-span are likely to be included in the context of a given question.  probability of words belong to the context, which is used as a soft-label for the auxiliary context prediction task.  Hypothesis: words in an answer-span are included in the context and make the probability of adjacent words decrease wit ha specific ratio as the distance between answer-span and a word increases.
    
  Block Attention
    Calculate segment embedding to predict soft-label, and localizes teh correct index of an answer-span with passage segment embedding.
    embedding spatial locality of passage segments to block attention models with following steps
      1. predicting start and end indices of context
      2. calculating passage segment with cumulaitve distribution of start and end tokens.
    
![image](https://user-images.githubusercontent.com/36841216/132169723-f0a59dd0-ebbc-4eb9-9b5f-e55fd55b50c8.png)    
![image](https://user-images.githubusercontent.com/36841216/132169669-b06fd2dc-3824-442b-bf22-eb5d0c09b42d.png)

    
