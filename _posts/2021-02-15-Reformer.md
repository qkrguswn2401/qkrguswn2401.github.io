---
title: "REFORMER: THE EFFICIENT TRANSFORMER"
date: 2021-02-16 08:26:28 -0400
categories: Paper Review
---
Abstract:
  Training transformer models can be costly especially on long sequences.   This paper introduces two techniques to efficiently train transformer model.  1: replacing dot-product with locality-sensitive hashing, this chainging complexity from O(L<sup>2</sup>) to O(Llog(L)). 2: reversible residual layers instead of standard residual.
Introduction:

Locality-sensitive hashing attention

Reverisble Transformer

Related Works
