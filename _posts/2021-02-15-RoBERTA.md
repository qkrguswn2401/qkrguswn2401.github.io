---
title: "Breaking Down Roberta"
date: 2021-02-16 08:26:28 -0400
categories: Paper Review
---
Problems with BERT: BERT is significantly undertrained because only 15% of training data are masked and learned.  New Language model, RoBERTa is introduced; it exceeds or match all of post-BERT methods.
What is Proposed: Here are four differences. 1: RoERTa is trained with longer and bigger batches over more data.  2:  next sentence prediction is removed 3: traind with longere sequences. 4. Masking pattern method is different (Dynamically changed), 5. CC-News dataset is introduced

How does Bert Learn?  loss is calcuated for for these two tasks

1. Masked Langauge Model: 15% of training data is selected randomly and masked.  of 80%, it is replaced with [Mask], of 10, uncahged, the rest is randomly selected from vocabulary.
2. What is Next Sentencen Prediction: 두 문장이 연결되었는지

Why four of these modifications are made.
1:  In NMT, training with larger and longer batches has shown improvement in opimization and performance when learning rate is higher.  Perplexity is lower when training data is a bit bigger, but not necessarily.

2.  Removing NSP loss matches or outperformes in downstream task performance.

3. subword-based BPE is applied.  Becasues it uses bytes, it allows to leran subword vocabulary with smaller size.   Less parameters

4. BERT masks statically, meaning same token is masked, which mean we are predicting same word token for every epoch.  To avoid static masking, training data is duplicated 10X so different token is masked for same data.  For dynamic masking, different pattern is applied for each feed forward.  
