---
title: "Stacked DeBert"
date: 2021-03-13 08:26:28 -0400
categories: Paper Review Debert
---

Problems: incorrect 올 missing data가 존재하는 text data 존재할시, 올바른 문장에 비해 performance 가 떨어진다.  요즘 방식들은 clean or complete data로 트레인 하기때문에 inadequate한 text에 대하여 좋은 feature를 뽑지못한다.  

Proposition: intermediate input representation을 input 토큰에 embedding layer와 vanilla transformer 적용해서 구하고.  여기서 나온 feature를 denoising transformer에 input으로 넣어 더 rich한 input representation으로 만든다. 여러 perceptron layer를 쌓은것으로 missing words에 word embedding을 reconstruction한다.  

Introduction: ASR을 거쳐 나온 text, writer 들이 항상 문법적을 맞는 문장을 쓴다고 생각하는건 어리석다.  Twitter같은 social media를 예로 들면 text들은 더 짧고, 간략해졌으며 grammar에 신경을 덜 쓰게되었다.  이러한 문제때문에 incomplete data에서 연구가 진행되었다. twit데이터 같은경우 noisy하여서 pro-processing 과정을 거쳐 data를 normalize하거나, encoder-decoder 구조를 이용하여 data를 reconsctruction하였다.   이 논문에서는 ASR 또는 twit에서 나온 noisy data (incorrect or missing) 를 어떻게 처리하는지 보여준다.  noisy data가 인풋으로 들어오니 이 task를 reconstruction task로 접근하여, noisy data-> clean data 어떻게? input을 meaningful feature에 mapping하여.  여기서 제시한 model은 BERT의 능력과 noisy data에 denoising transformer를 이용하여 reconstruction을 해 task 1 유저가 intention이 무엇 task 2 classification 문제를 풀어낸다.  

Stacked DeBERT 구조: 이 모델은 denoising transformer와 vanilla transformer 그리고 embedding layer로 크게 구성되어있다.  embedding layer와 vanilla transformer를 거친 input은 intermedate representation이며 denoiser transformer에 input이 된다.  이 denoiser representation을 나온 feature 은 좀더 좋은 정보를 가지고있는 feature가 된다.  Denoising layer를 통하여 incomplete -> complete data로 reconstruction되며, 이 feature로 classification을 진행한다. (더 Robust 한 [CLS] token이 만들어지게 된다)

Training시: model은 incomplete, complete data 두개다 인풋으로 쓰며 denoiser transformer layer에서 reconsctruction을 진행하여 intermediate embedding을 만들고, 이것이 BERT이 인풋으로 들어가 final embedding in h(rec)을 만든다.  
Test: incomplete 한 data 쓴다.


Why bert is powerful?
1. 2 objectives: NSP, MLM
2. NSP: 연결되는 두 문장이 순서가 맞는지
3. MLM: input data의 15%의 token mask해 generation 해 맞추는것 (language modeling)

문제점: BERT는 incorrect data 대하여 아무것도 하지않는다 -> noisy data 들어올시 performance가 줄어듬.

Proposed Model:
1. text data를 proprocessing
  1-1. lower case 전환 후 tokenize
  1-2. [ClS] [tok1] [tok2....] [SEP] 붙이기
2.embedding layer, vobulary를 이용하여 맞는 vector에 mapping
3. noisy data를 처리하는 과정이없어서 performance가 낮은데, 이부분은 denoising transformer에서 처리해준다. training: both complete and incomplte ; test: incomplete
4. denoising transformer를 학습하기위해 2 step process가있다:
  4-1. stacked multi perceptron이 intermediate feature이 h'(rec)을 더 의미있고, 복구된 reconsctruct하게 train, Loss : complete 과 h'(rec)을 비교해서 figure 1.
  4-2. h'(rec)을 Bidirectional transform의 인풋으로해서 마지막 final embedding 나오게.
 

![image](https://user-images.githubusercontent.com/36841216/109807664-8766be80-7c69-11eb-8701-b471ab9886c9.png)
Figure 1
![image](https://user-images.githubusercontent.com/36841216/109803639-a4e55980-7c64-11eb-920a-c7881b4d29c8.png) Figure 2
