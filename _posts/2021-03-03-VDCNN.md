---
title: "Very Deep Convolutional Networks for Text Classification"
date: 2021-03-03 08:26:28 -0400
categories: Paper Review NLP
---
Abstract: NLP분야에서 CNN의 깊이는 CV에서 사용되는 CNN 깊이보다 shallow 했다.  VDCNN은 VGG16 처럼 kernel 사이즈를 작게하고, 깊게 했을시, 
몇 public classifcication dataset에서 sota 성능을 보였다.  Deep CNN 이 text에 처음으로 사용되었다. 

Introduction:
token 은 sequential하기때문에 (Left -> Right) NLP 분야에서는 RNN이 많이 쓰여진다.  sequence는 RNN (LSTM)의 hidden로 기억된다.
LSTM은 sequence를 처리하기위한 generic한 방법이지만, task-specific 하지 않다.  
Text processing에서 ConvNets을 사용한 사례는 있었지만, 이번에 제시된 VDCNN은 이전 ConvNets보다 더 좋은 성능을 보여주고있따.


Related Work:
1


![image](https://user-images.githubusercontent.com/36841216/109649190-e9a6bd00-7b9e-11eb-96fb-391d2a05748e.png)
VDCNN 구조
![image](https://user-images.githubusercontent.com/36841216/109649527-646fd800-7b9f-11eb-8cde-49d56da70622.png)
ConvBlock 구조
