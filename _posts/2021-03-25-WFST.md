---
title: "Weighted Finite-State Transduce(WFST)"
date: 2021-03-25 08:26:28 -0400
categories: Speech-Recognition
---
Motivation: Simple HMM model with small vocab can be decoded with Viterbi decoding.  When vocabulray is large or input data is big, models become too complex to be solved unlike the HMM with small vocabulary size.  

WFST is compoesed of  multiple transducers to a signle transducer and uses it to search for a grammtically-sound sentence.
It provides a natural representation for HMM phonetic models, context-dependent phones, pronunciation lexicons, and grammars.




Weights are associated with edges as costs or probability.
WFST composes transducers together to build structures.  Transducer encode a mapping between the input and the output label sequences.  
From the table below, HMM trasducers convert HMM states into context-depedent phones. For each part, transducers are combined -> decoder is created.  

<br/> transducer L (pronunciation lexicon transducer):
![image](https://user-images.githubusercontent.com/36841216/112437359-3474b880-8d8a-11eb-9720-55540060e566.png)
pronunciation lexicon transducer을 이용해 input phone로 받아 words output을 만들어낸다.  ex) /b/ /ih/ /l/ phone을 받아 bill이라는 word 아웃풋을 뽑아낸다.
![image](https://user-images.githubusercontent.com/36841216/112440531-c631f500-8d8d-11eb-8fda-ff92c0c2d990.png)

Transducer G:
![image](https://user-images.githubusercontent.com/36841216/113383793-a28b3200-93bf-11eb-8f30-f74b3ef7745d.png)
마지막 state으로, word마다 likehood를 가지고있으며, 단얻르이 순서로 연결되야 의미, 문법적으로 맞는지 확률로 계산.

<br/> Finite State Automaton (Finite State Machine)
![image](https://user-images.githubusercontent.com/36841216/112444018-efa05000-8d90-11eb-843b-f97f347bbff5.png)
Input a is received with wiehgt 0.6  (first state)  output: e is passed to next state with the weight.
