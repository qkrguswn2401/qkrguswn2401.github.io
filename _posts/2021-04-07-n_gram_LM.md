---
title: "n_gram_LM"
date: 2021-04-08 08:26:28 -0400
categories: study
---
Study for N-gram:
What are n-grams: models that assign the probabilities to sentences or sequences of words, the n-gram.  
calculating  bi-grams:
![image](https://user-images.githubusercontent.com/36841216/113851414-7401c280-97d6-11eb-84ed-308835872a49.png)

<br/> General Equation for bi_gram: ![image](https://user-images.githubusercontent.com/36841216/113850713-b545a280-97d5-11eb-91f1-484cb3e13acb.png)
<br/> example of bi-gram: 
![image](https://user-images.githubusercontent.com/36841216/113851925-f25e6480-97d6-11eb-9ef4-6058a985213f.png)
![image](https://user-images.githubusercontent.com/36841216/114139709-56ee0080-994a-11eb-8ecc-c4cd82f6de05.png)

![image](https://user-images.githubusercontent.com/36841216/114139533-227a4480-994a-11eb-813f-11923b78b2a4.png)


In practive, it is common to use trigram models, or 4-gram/5 grams when there is sufficient data.
when computing tri-gram at the very beginning of sentence, use two pseudo-wods
![image](https://user-images.githubusercontent.com/36841216/113852500-921bf280-97d7-11eb-9426-89a6a07d9fe4.png)

<br/>log probability format is used when computing language model probability because mulltiplying n-grams would result in nummerical underflow.  
adding in logspace is equal to multiplying in linear space ->  combine log probabilities by adding them.
![image](https://user-images.githubusercontent.com/36841216/113853763-00ad8000-97d9-11eb-9674-17c0a7390b71.png)

Evaluation of language model:
<br/>Extrinsic Evaluation: way to know if a particular improvment in a component is going to help specific task.
<br/>Intrinsic Evaluation: one that measures the quality of a language model independent of any application. (Ex) caluculating perplxity

Perplexity: way to evaluate a language model, perplexity of a language model model on a test set is the inverse probability of the test set, normalized by the nuber of words. 
Exampel)
<br/>![image](https://user-images.githubusercontent.com/36841216/113855886-85010280-97db-11eb-9b21-1039bf1a42d8.png)
Applying chain rule to expand the probability of W:
<br/>![image](https://user-images.githubusercontent.com/36841216/113856086-bc6faf00-97db-11eb-8e58-e7f26519c207.png)
Minimizing perplxity means maximizing the test set probability according to the language model.  

When computing perplxity of W with a bigram model.
![image](https://user-images.githubusercontent.com/36841216/113855915-8c281080-97db-11eb-8dca-793323911ef1.png)

Example of Calculating Perplxity:
we are to recognize digits (zero to nine) probability = 1/10
<br/>![image](https://user-images.githubusercontent.com/36841216/113856492-47e94000-97dc-11eb-846a-140bbb260cf0.png)

Calculated perplexity on wall_street journal dataset.  
![image](https://user-images.githubusercontent.com/36841216/113859830-4c175c80-97e0-11eb-9508-0d19e8ea4b5e.png)
longer n-gram gives more information -> lowering the perplexity.
We can think of ppl as branching factor.
For example if ppl is 20, it mean model is consiring possible 20 word at each time step.
If two language models are needed to be compared, they must share same vocabulary.
