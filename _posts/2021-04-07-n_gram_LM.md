---
title: "n_gram_LM"
date: 2021-04-08 08:26:28 -0400
categories: study
---
Study for N-gram:
What are n-grams: model taht assigns the probabilities to sentences or a sequences of words, the n-gram.  
Equation for n-grams:
![image](https://user-images.githubusercontent.com/36841216/113851414-7401c280-97d6-11eb-84ed-308835872a49.png)

<br/> General Equation for n_gram: ![image](https://user-images.githubusercontent.com/36841216/113850713-b545a280-97d5-11eb-91f1-484cb3e13acb.png)
<br/> example of bi-gram: 
![image](https://user-images.githubusercontent.com/36841216/113851925-f25e6480-97d6-11eb-9ef4-6058a985213f.png)
Given a probability for a bi-gram, we can calculate probability for a sentence.
![image](https://user-images.githubusercontent.com/36841216/113851986-0013ea00-97d7-11eb-82fc-5f5112c0295c.png)

In practive, it is common to use trigram models, or 4-gram/5 grams when there is sufficient data.
when computing tri-gram at the very beginning of sentence, use two pseudo-wods
![image](https://user-images.githubusercontent.com/36841216/113852500-921bf280-97d7-11eb-9426-89a6a07d9fe4.png)

<br/>log probability format is used when computing language model probability because mulltiplying n-grams would result in nummerical underflow.  
adding in logspace is equal to multiplying in linear space ->  combine log probabilities by adding them.
![image](https://user-images.githubusercontent.com/36841216/113853763-00ad8000-97d9-11eb-9674-17c0a7390b71.png)

Evaluation of language model:
Exrinsic Evaluation: way to know if a particular improvment in a component is going to help specific task.
Intrinsic Evaluation: one that measures the quality of a language model independent of any application.

Perplexity: way to evaluate a language model, perplexity of a language model model on a test set is the inverse probability of the test set, normalized by the nuber of words. 
Exampel)
![image](https://user-images.githubusercontent.com/36841216/113855886-85010280-97db-11eb-9b21-1039bf1a42d8.png)
Applying chain rule to expand the probability of W:
![image](https://user-images.githubusercontent.com/36841216/113856086-bc6faf00-97db-11eb-8e58-e7f26519c207.png)
Minimizing perplxity means maximizing the test set probability according to the language model.  

When computing perplxity of W with a bigram model.
![image](https://user-images.githubusercontent.com/36841216/113855915-8c281080-97db-11eb-8dca-793323911ef1.png)

Example of Calculating Perplxity:
we are to recognize digits (zero to nine) probability = 1/10
![image](https://user-images.githubusercontent.com/36841216/113856492-47e94000-97dc-11eb-846a-140bbb260cf0.png)


