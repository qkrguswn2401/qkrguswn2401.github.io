---
title: "Longformer: The Long-Document Transformer"
date: 2022-02-28 08:26:28 -0400
categories: Paper Review
---
Longformer: The Long-Document Transformer

Abstract:
  Transformer-based models are unable to process long sequences due to self-attention operation (not scalable).  Longformer overcomes scalability issue by introducing attention mechanism which increases linearly with sequence length
    
