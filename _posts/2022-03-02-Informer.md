---
title: "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"
date: 2022-03-12 08:26:28 -0400
categories: Paper Review
---

Abstract:
  Long Sequence time-series forecast (LSTF) demands high prediction capacity of model.  Transformer has shown potential to increase prediction capacity. There are a few issues with Transformer: quadratic time complexity, high memory usage, encoder-decodr architecture.  An efficient transformer-based model for LSTF is introduced, Informer, which has three characteristics: Probsparse self-attention, self-attention distilling highlights dominating attentio nby havling cascading layer input and handles long input, generative style decoder.  
  
Introduction:
  The major challenge for LSTF is to enhance the prediction capacity to meet the increasingly long sequence demand, which requires extraordinary long-range alignment and efficient operations on long sequence inputs and outputs.  Transformer model has shown superior performanec in capturing long-range dependency than RNN models.  Self-attention mechanism is L-quadratic computation and memory comsumption.  Very large-scale Transformer models has been shown successful results on NLP tasks but unaffordable on real-world LSTF problems.  This paper talks about improving Transformer model to be computation, memory, and architecture efficient while maintaining higehr prediction capacity.  Vanilla transformer has three limitation: (1) the quadratic computation of self-attention (2) The memory bottleneck in stacking layer for long inputs (3) the speed drops very low in predicting long ouputs.  
  
![image](https://user-images.githubusercontent.com/36841216/156305104-4aaadaa5-6220-4c7b-9ba2-25c1b5c5a236.png)
</br>Methodology:  Existing models for time-series forecasting can be roughly grouped into two categories: classical time-series, deep learning technique (Encode-decoder).  

  Efficient Self-attention Mechanism: self-attention from Transformer performs the scaled dot-product, Softmax(QK<sup>T</sup>/sqrt(d)V) where d is the input dimension. i-th query's attention is defined as a kernel smoother in a probability form: ![image](https://user-images.githubusercontent.com/36841216/156296545-9dc1de2f-1cd9-4d88-901f-dfc4f866adb8.png)  Quadratic dot-product computation and O(L<sup>Q</sup>L<sup>K</sup>).  Self-attention has potential sparsity issue.  
  
    Query Sparsity Measurement: The dominant dot-product pairs encourage the corresponding query's attention probability distribution away from the uniform distribution.  Query's sparsity measurement:![image](https://user-images.githubusercontent.com/36841216/156299001-dc94b9bc-93d6-4f52-b8f1-19b0ef82fb34.png)  If the i-th query gains a larger M(q<sup>i</sup>,K), its attention probability p is more "diverse" and has high chance to contain the dominate dot-product pairs.
    
    ProbSparse Self-attention: This allows each key to attend to the only u domiante queries  ![image](https://user-images.githubusercontent.com/36841216/156299375-dab19004-7074-4aa5-8692-594b14fc1f24.png)  Q-bar contains only top-u queries under the sparsity measurement where u = c*ln(L<sub>Q</sub> c=sampling factor).  ProbSparse self-attention only need to calculate O(ln(L<sub>Q</sub>).  Thus dot-product computation reduces to O(L<sub>K</sub>*ln(L<sub>Q</sub>).  In practice, length of queries and keys are equivalent in length, L<sub>Q</sub>=L<sub>K</sub>=L; ProbSparse self-attention time complexity and sapce complexity O(L*ln(L))
  ![image](https://user-images.githubusercontent.com/36841216/156300376-08e8ae21-75bc-45f5-9d91-23cac54857fd.png)
  Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation
  Encoder is designed to extract the robust long-range dependency of the long sequential inputs.
  
    Self-attention Distilling: Due to ProbSparse self-attention mechanism, encoder's feature map has redundant combinations of value V. Distilling operation is used to make use of dominating features and make a focused self-attention feature map in the next layer.  this distilling procedure forwards from j-th layer into (j+1)-th layer as ![image](https://user-images.githubusercontent.com/36841216/156304314-74d4c086-9a36-4416-8aca-596efa98f065.png) [] represents the attention block, which contains multi-head ProbSparse self-attention and the essentiual operations.  Max-pooling layer with stride 2 and downsample input into its half slice after stacking layer, which reduces memory usage to be O(2-e)Llog(L)) where e is very small number.  To enhance the robustness of the distilling operation, number of self-attention distilling layers are decreased by dropping one layer at a time.  Final hiddne representation of encoder is obtained by concatenating all the stacks' outputs.  
    
  Decoder: Generating Long Sequential Outputs Through One Forward Procedure
    Standard transformer decoder is used, composed of a stack of two identical multi-head attention layers.  Generative inference is employed to alleviate the speed plunge in long prediction.
      Genrative Inference: Instead of choosing specific token as the token, long sequence is used as a token such as earlier slice before the output sequence.  Example) take 7-days temperature prediction.  We will use the whole known 5 days information as the start token predict 2 days.
      Loss function: MSE function is used
