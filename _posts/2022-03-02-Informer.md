---
title: "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"
date: 2022-03-12 08:26:28 -0400
categories: Paper Review
---

Abstract:
  Long Sequence time-series forecast (LSTF) demands high prediction capacity of model.  Transformer has shown potential to increase prediction capacity. There are a few issues with Transformer: quadratic time complexity, high memory usage, encoder-decodr architecture.  An efficient transformer-based model for LSTF is introduced, Informer, which has three characteristics: Probsparse self-attention, self-attention distilling highlights dominating attentio nby havling cascading layer input and handles long input, generative style decoder.  
  
Introduction:
  The major challenge for LSTF is to enhance the prediction capacity to meet the increasingly long sequence demand, which requires extraordinary long-range alignment and efficient operations on long sequence inputs and outputs.  Transformer model has shown superior performanec in capturing long-range dependency than RNN models.  Self-attention mechanism is L-quadratic computation and memory comsumption.  Very large-scale Transformer models has been shown successful results on NLP tasks but unaffordable on real-world LSTF problems.  This paper talks about improving Transformer model to be computation, memory, and architecture efficient while maintaining higehr prediction capacity.  Vanilla transformer has three limitation: (1) the quadratic computation of self-attention (2) The memory bottleneck in stacking layer for long inputs (3) the speed drops very low in predicting long ouputs.  

</br>Methodology:  Existing models for time-series forecasting can be roughly grouped into two categories: classical time-series, deep learning technique (Encode-decoder).  

  Efficient Self-attention Mechanism: self-attention from Transformer performs the scaled dot-product, Softmax(QK<sup>T</sup>/sqrt(d)V) where d is the input dimension.
  
  Query Sparsity Measurement
  
  ProbSparse Self-attention
  
  Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation
    Self-attention Distilling:
  Decoder: Generating Long Sequential Outputs Through One Forward Procedur
    Loss function
