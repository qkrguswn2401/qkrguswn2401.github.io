---
title: "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"
date: 2022-03-12 08:26:28 -0400
categories: Paper Review
---

Abstract:
  Long Sequence time-series forecast (LSTF) demands high prediction capacity of model.  Transformer has shown potential to increase prediction capacity. There are a few issues with Transformer: quadratic time complexity, high memory usage, encoder-decodr architecture.  An efficient transformer-based model for LSTF is introduced, Informer.  
Introduction:

Preliminary:

Methodology:
  Efficient Self-attention Mechanism
  
  Query Sparsity Measurement
  
  ProbSparse Self-attention
  
  Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation
    Self-attention Distilling:
  Decoder: Generating Long Sequential Outputs Through One Forward Procedur
    Loss function
