---
title: "Breaking Down Roberta"
date: 2021-02-15
categories: Paper Review
---

Problems with BERT: BERT is significantly undertrained because only 15% of training data are masked and learned.  New Language model, RoBERTa is introduced; it exceeds or match all of post-BERT methods.
What is Proposed: 
1:
